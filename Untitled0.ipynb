{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#This notebook is being to train the finetuned squad model"
      ],
      "metadata": {
        "id": "VTh3WoXVtjXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libs\n",
        "!pip install datasets\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip instal torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vawBUq4voic",
        "outputId": "cccfacc4-67ba-4db4-c093-bc98af372c1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"tootooba/SMU_faq_dataset\")"
      ],
      "metadata": {
        "id": "uczgTU0Etg9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the model (directly--> more flexible & customizable)\n",
        "# Load model directly\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-cased-distilled-squad\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IlqgS7uNuScc",
        "outputId": "1eb2fd42-66ed-429f-9940-897e535a2424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# preparing the dataset for training, this includes\n",
        "# 1) extracting the qas from the ds\n",
        "# 2) tokenzing these extracted qas using the tokenizer imported in the prev cell\n",
        "\n",
        "\n",
        "# 1)\n",
        "# \"train\" because there is no test data in the ds, you wouldve seen this when\n",
        "# you ran the second block of code\n",
        "questions = ds['train']['question']\n",
        "answers = ds['train']['answer']\n",
        "\n",
        "# 2)\n",
        "# for the parameters, check the website:\n",
        "# https://huggingface.co/docs/transformers/en/main_classes/tokenizer\n",
        "\n",
        "tokenized_data = tokenizer(\n",
        "    questions,\n",
        "    answers,\n",
        "    # padding to equalize length (longest because we dont want any of the answers)\n",
        "    padding = \"longest\",\n",
        "    #truncation: ensures that inuts are not longer than the max length\n",
        "    truncation = True,\n",
        "    max_length = 512,\n",
        "    # pt--> pyTorch tensors, tensors are essentially a generalisation of matrices\n",
        "    return_tensors  =\"pt\"\n",
        ")"
      ],
      "metadata": {
        "id": "XsvivOCBvfW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Prepare Labels for Question Answering\n",
        "# Extract the start and end positions of each answer in the corresponding context (question)\n",
        "\n",
        "start_positions = []\n",
        "end_positions = []\n",
        "\n",
        "for question, answer in zip(questions, answers):\n",
        "    start_idx = question.find(answer)\n",
        "    if start_idx == -1:\n",
        "        # If the answer is not found in the question, set default values\n",
        "        start_positions.append(0)\n",
        "        end_positions.append(0)\n",
        "    else:\n",
        "        start_positions.append(start_idx)\n",
        "        end_positions.append(start_idx + len(answer))\n",
        "\n",
        "# Step 2: Tokenize Data with Labels\n",
        "# Include the start and end positions\n",
        "tokenized_data = tokenizer(\n",
        "    questions,\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=1024,\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "# Add start and end positions to the tokenized data\n",
        "tokenized_data['start_positions'] = torch.tensor(start_positions)\n",
        "tokenized_data['end_positions'] = torch.tensor(end_positions)\n",
        "\n",
        "# Step 3: Create Dataset from Tokenized Data\n",
        "print(\"Preparing dataset for training...\")\n",
        "qa_dataset = Dataset.from_dict({\n",
        "    \"input_ids\": tokenized_data['input_ids'],\n",
        "    \"attention_mask\": tokenized_data['attention_mask'],\n",
        "    \"start_positions\": tokenized_data['start_positions'],\n",
        "    \"end_positions\": tokenized_data['end_positions']\n",
        "})\n",
        "\n",
        "# Step 4: Set Up Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=qa_dataset,\n",
        "    eval_dataset=qa_dataset,    # Using the same dataset for evaluation (no separate test set)\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics  # Function to compute metrics like accuracy\n",
        ")\n",
        "\n",
        "# Step 5: Train the Model\n",
        "print(\"Training the model...\")\n",
        "trainer.train()\n",
        "\n",
        "# Step 6: Evaluate and Print Results after Each Epoch\n",
        "print(\"Evaluating the model...\")\n",
        "metrics = trainer.evaluate()  # Evaluate on the training data\n",
        "print(f\"Metrics after training: {metrics}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "Sa-PzJ3dzqhQ",
        "outputId": "19a479e8-c931-427f-9e8d-607297845e02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing dataset for training...\n",
            "Training the model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='90' max='90' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [90/90 07:00, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.030700</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:27]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics after training: {'eval_loss': 0.0, 'eval_accuracy': 1.0, 'eval_runtime': 28.2434, 'eval_samples_per_second': 8.498, 'eval_steps_per_second': 1.062, 'epoch': 3.0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of training dataset: {len(qa_dataset)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9vBetoPA5cGQ",
        "outputId": "8493f1e4-f707-4ed8-ff85-f7f50253b437"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of training dataset: 240\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract original questions and answers from the dataset\n",
        "original_questions = ds['train']['question']\n",
        "original_contexts = ds['train']['answer']\n",
        "\n",
        "# Test the model using the original questions\n",
        "for i, (question, context) in enumerate(zip(original_questions, original_contexts)):\n",
        "    # Tokenize the question and context\n",
        "    inputs = tokenizer(\n",
        "        question,\n",
        "        context,\n",
        "        add_special_tokens=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Get predictions from the model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract start and end logits\n",
        "    start_logits = outputs.start_logits\n",
        "    end_logits = outputs.end_logits\n",
        "\n",
        "    # Get the most likely start and end of the answer\n",
        "    start_index = torch.argmax(start_logits)\n",
        "    end_index = torch.argmax(end_logits)\n",
        "\n",
        "    # Convert tokens to the answer text\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
        "    answer_tokens = all_tokens[start_index:end_index + 1]\n",
        "    predicted_answer = tokenizer.convert_tokens_to_string(answer_tokens)\n",
        "\n",
        "    # Print the original question and model prediction\n",
        "    print(f\"Original Question {i+1}: {question}\")\n",
        "    print(f\"Predicted Answer: {predicted_answer}\\n\")\n",
        "\n",
        "    # Stop after a few examples to limit output\n",
        "    if i == 4:\n",
        "        break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FQ3AaFrE8KVU",
        "outputId": "7d6c3923-c3fb-4247-8525-28809bc167fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Question 1: ACADEMIC CALENDAR: That is it? Why do I need it? Now do I get one?\n",
            "Predicted Answer: [CLS]\n",
            "\n",
            "Original Question 2: ATHLETIC EVENTS: There can I get tickets? Now much do they cost? Are discount available? Now do I find more information?\n",
            "Predicted Answer: [CLS]\n",
            "\n",
            "Original Question 3: BUS PASS: There do I get a pass for Retro Transit? Now much does it cost? Are there special arrangements for students with disabilities? There can I find transit schedules and route maps?\n",
            "Predicted Answer: [CLS]\n",
            "\n",
            "Original Question 4: CAMPUS TOURS: Now do I arrange a tour of the camps? Now far in advance do I need to book?\n",
            "Predicted Answer: [CLS]\n",
            "\n",
            "Original Question 5: COMPUTER ACCOUNTS: I have received an 'A' number. That is it? Now do I use it? That is the Self-Service Manner? Now do I access it? Now do I get an 's' number? There can I find more information?\n",
            "Predicted Answer: [CLS]\n",
            "\n"
          ]
        }
      ]
    }
  ]
}